{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Implementing and Analyzing a Basic RL Algorithm\n",
        "\n",
        " **Components of the Markov Decision Process (MDP):**\n",
        " 1. *States (S):* Each cell in the grid represents a state. The agent can be in any cell except for the blocked cells. Let's denote the states by their grid coordinates (i, j).\n",
        " 1. *Actions (A):* The agent can move in four directions:\n",
        "  * North (N)\n",
        "  * South (S)\n",
        "  * East (E)\n",
        "  * West (W)\n",
        " 1. *Transition Probability (P):* The agent moves in the chosen direction with a probability of 1‚àínoise. With probability noise, it may move in a different direction. For simplicity, let's assume the noise is evenly distributed among the other three directions.\n",
        " 1. *Reward Function (R):* The reward structure is as follows:\n",
        "  * Reward of 0 for moving into any non-terminal and non-blocked cell.\n",
        "  * Reward of -5 for moving into a fire cell (red cell).\n",
        "  * Reward of +5 for moving into the goal cell (G).\n",
        " 1. *Discount Factor (ùõæ):* The discount factors to be considered are 0.95 and 0.75.\n",
        " 1. *Termination:* The episode ends when the agent reaches the goal cell (G) or a fire cell (red cell)."
      ],
      "metadata": {
        "id": "FS5BFq6x0pud"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-SOCfgJtHJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e341d9-ca25-4bd5-ad0f-ebc6276d820b"
      },
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import matplotlib.colors as mcolors\n",
        "from tabulate import tabulate"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-5bbb80424a53>:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-notebook')\n",
            "<ipython-input-1-5bbb80424a53>:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-whitegrid')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Linear Solver Method"
      ],
      "metadata": {
        "id": "yQq2YuOsy54c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Action(IntEnum):\n",
        "    \"\"\"The class Action represents the set of possible actions in gridworld.\"\"\"\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "# String representation for each action saved numerically (easier for human interpertation)\n",
        "action_to_str = {\n",
        "    Action.up: \"up\",\n",
        "    Action.right: \"right\",\n",
        "    Action.down: \"down\",\n",
        "    Action.left: \"left\",\n",
        "}\n",
        "\n",
        "# How locations should be updated given any of the 4 actions\n",
        "action_to_offset = {\n",
        "    Action.up: (-1, 0),\n",
        "    Action.right: (0, 1),\n",
        "    Action.down: (1, 0),\n",
        "    Action.left: (0, -1),\n",
        "}\n",
        "\n",
        "# For printing the found policy\n",
        "action_symbols = {\n",
        "    Action.up: \"^\",\n",
        "    Action.right: \">\",\n",
        "    Action.down: \"v\",\n",
        "    Action.left: \"<\"\n",
        "}"
      ],
      "metadata": {
        "id": "ZXQ368niMllC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=5.0, danger=[], danger_value=-5.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [0 for _ in range(height * width)]\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise  # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1  # Ensure valid noise value.\n",
        "        self.create_next_values()  # Initialize the next state values.\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [0 for _ in range(self._height * self._width)]\n",
        "        self.create_next_values()\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state >= 0 and state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state_r >= 0 and state_r < self._height and state_c >= 0 and state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        # Get current state indecies\n",
        "        row, col = self._state_to_rc(state)\n",
        "        # Find the new indecies based on this action\n",
        "        offset_row, offset_col = action_to_offset[action]\n",
        "        next_row, next_col = row + offset_row, col + offset_col\n",
        "        # Translate indecies to a state\n",
        "        next_state = next_row * self._width + next_col\n",
        "        # Assure the calculated state is acceptable aka. within boundaries and not an obstacle\n",
        "        if self._inbounds_rc(next_row, next_col):\n",
        "            if next_state not in self._blocked_cells:\n",
        "                return next_state\n",
        "        return state\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        return state == self._goal_cell or state in self._danger_cells\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment\n",
        "        \"\"\"\n",
        "        return [s for s in range(self._height * self._width) if s not in self._blocked_cells]\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state\n",
        "        \"\"\"\n",
        "        return [Action.up, Action.right, Action.down, Action.left]\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        # Reward is non-zero for danger or goal\n",
        "        if state in self._danger_cells:\n",
        "            return self._danger_value\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        return 0\n",
        "\n",
        "    def get_transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Get a list of transitions as a result of attempting the action in the current state\n",
        "        Each item in the list is a dictionary, containing the probability of reaching that state and the state itself\n",
        "        \"\"\"\n",
        "        # When the state is terminal then only one probable action is available\n",
        "        if self.is_terminal(state):\n",
        "            return [{'prob': 1.0, 'state': state}]\n",
        "        transitions = []\n",
        "        primary_state = self._state_from_action(state, action)\n",
        "        # Probability that the agent wants to move to state s' and it actually moved there\n",
        "        transitions.append({'prob': 1 - self._noise, 'state': primary_state})\n",
        "        if self._noise > 0:\n",
        "            # from the actions list find valid states that are not s'\n",
        "            other_actions = [a for a in self.get_actions(state) if a != action]\n",
        "            noise_prob = self._noise / len(other_actions)\n",
        "            for other_action in other_actions:\n",
        "                secondary_state = self._state_from_action(state, other_action)\n",
        "                # Given the noise: probability that the agent didn't move to the direction in the decision\n",
        "                transitions.append({'prob': noise_prob, 'state': secondary_state})\n",
        "        return transitions\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Get the current value of the state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        return self._grid_values[state]\n",
        "\n",
        "    def create_next_values(self):\n",
        "        \"\"\"\n",
        "        Creates a temporary storage for state value updating\n",
        "        If this is not used, then asynchronous updating may result in unexpected results\n",
        "        To use properly, run this at the start of each iteration\n",
        "        \"\"\"\n",
        "        self._next_values = self._grid_values.copy()\n",
        "\n",
        "    def set_next_values(self):\n",
        "        \"\"\"\n",
        "        Set the state values from the temporary copied values\n",
        "        To use properly, run this at the end of each iteration\n",
        "        \"\"\"\n",
        "        self._grid_values = self._next_values.copy()\n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        \"\"\"\n",
        "        Set the value of the state into the temporary copy\n",
        "        This value will not update into main storage until self.set_next_values() is called.\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        self._next_values[state] = value\n",
        "\n",
        "    def _state_to_index(self, states):\n",
        "        \"\"\"Initiate a dictionary maps a state to its index\"\"\"\n",
        "        return {s: idx for idx, s in enumerate(states)}\n",
        "\n",
        "    def solve_linear_system(self, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Solve the gridworld using a system of linear equations.\n",
        "        :param discount_factor: The discount factor for future rewards.\n",
        "        \"\"\"\n",
        "        # To solve the linear system of equations\n",
        "        #   find the values of metrices A and B\n",
        "\n",
        "        # Initialize metrices\n",
        "        num_states = self._width * self._height\n",
        "        A = np.zeros((num_states, num_states))\n",
        "        b = np.zeros(num_states)\n",
        "\n",
        "        # Assign values to A and B\n",
        "        for state in range(num_states):\n",
        "            # When the state is at an obstacle or terminal don't calculate an s(v) it's always the same as reward\n",
        "            if state in self._blocked_cells or self.is_terminal(state):\n",
        "                A[state, state] = 1.0\n",
        "                b[state] = self.get_reward(state)\n",
        "            else:\n",
        "              # If it's not terminal nor obstacle, calculate its values\n",
        "              A[state, state] = 1.0\n",
        "              for action in self.get_actions(state):\n",
        "                  # v(s) = P(s'|s,a)(r+)\n",
        "                  n_actions = len(self.get_actions(state))\n",
        "                  next_state = self._state_from_action(state, action)\n",
        "                  reward = self.get_reward(next_state)\n",
        "                  A[state, next_state] -= discount_factor / n_actions\n",
        "                  b[state] += reward / n_actions\n",
        "\n",
        "        # Solve the equation\n",
        "        V = np.round(np.linalg.solve(A, b), 2)\n",
        "\n",
        "        for state in range(num_states):\n",
        "            self.set_value(state, V[state])\n",
        "\n",
        "        return V.reshape((self._height, self._width))\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str"
      ],
      "metadata": {
        "id": "FNFooVyFMZjA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "\n",
        "print(\"Initial Grid:\")\n",
        "print(simple_gw)\n",
        "\n",
        "# Solve the linear system\n",
        "print(\"Solving with linear solver:\")\n",
        "values_grid = simple_gw.solve_linear_system()\n",
        "print(values_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiylKX0fMtZ5",
        "outputId": "2e0b4057-85fa-4d3d-baf1-e018bd39efe1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Grid:\n",
            "  0.00   0.00  -5.00   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   GOAL \n",
            "  0.00   0.00   0.00  -5.00   0.00 \n",
            "  0.00  -5.00   0.00   0.00   0.00 \n",
            "\n",
            "Solving with linear solver:\n",
            "[[-9.97 -9.99 -5.   -3.33  0.  ]\n",
            " [-9.96  0.    0.    0.    3.33]\n",
            " [-9.94  0.    0.    0.    5.  ]\n",
            " [-9.93 -9.88 -9.71 -5.   -1.6 ]\n",
            " [-9.96 -5.   -9.24 -8.02 -4.81]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial Grid:\")\n",
        "print(simple_gw)\n",
        "print(\"Solving with linear solver: (discount_factor=0.95)\")\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.95)\n",
        "print(values_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IORA29rlWs92",
        "outputId": "413c2a78-137a-43c5-d7c9-703990585e75"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Grid:\n",
            "  0.00   0.00  -5.00   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   GOAL \n",
            "  0.00   0.00   0.00  -5.00   0.00 \n",
            "  0.00  -5.00   0.00   0.00   0.00 \n",
            "\n",
            "Solving with linear solver: (discount_factor=0.95)\n",
            "[[-5.25 -7.02 -5.   -3.2  -0.  ]\n",
            " [-4.59  0.    0.   -0.    3.2 ]\n",
            " [-4.89  0.    0.   -0.    5.  ]\n",
            " [-6.22 -7.62 -8.   -5.   -1.1 ]\n",
            " [-7.46 -5.   -7.78 -6.72 -3.54]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial Grid:\")\n",
        "print(simple_gw)\n",
        "print(\"Solving with linear solver: (discount_factor=0.75)\")\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.75)\n",
        "print(values_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzPHzE1jQ6no",
        "outputId": "1fab1056-362a-46d1-a02e-b0495364c691"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Grid:\n",
            "  0.00   0.00  -5.00   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   GOAL \n",
            "  0.00   0.00   0.00  -5.00   0.00 \n",
            "  0.00  -5.00   0.00   0.00   0.00 \n",
            "\n",
            "Solving with linear solver: (discount_factor=0.75)\n",
            "[[-1.37 -3.91 -5.   -2.69  0.  ]\n",
            " [-0.66  0.    0.    0.    2.69]\n",
            " [-0.84  0.    0.    0.    5.  ]\n",
            " [-2.14 -4.29 -4.78 -5.   -0.3 ]\n",
            " [-4.14 -5.   -4.74 -4.09 -1.32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lOsaKM_Yyt3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Dynamic Programming Solver"
      ],
      "metadata": {
        "id": "lEkSmh_Eyv4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(gw, discount, tolerance=0.1):\n",
        "    \"\"\"Solve the gridworld using a dynamic programming method (value iteration algorithm)\"\"\"\n",
        "    gw.reset()\n",
        "    loops = 0\n",
        "    while True:\n",
        "        loops += 1\n",
        "        delta = 0\n",
        "        gw.create_next_values()\n",
        "        for state in gw.get_states():\n",
        "            # If the current state is a terminal with a predefined value, skip it.\n",
        "            if gw.is_terminal(state):\n",
        "                continue\n",
        "            action_values = []\n",
        "            # Calculate the value function at each action\n",
        "            #  Consider the current action as the prime action\n",
        "            for action in gw.get_actions(state):\n",
        "                action_value = 0\n",
        "                # Fidn the probabilty of all other actions given the current prime action\n",
        "                for transition in gw.get_transitions(state, action):\n",
        "                    next_state = transition['state']\n",
        "                    prob = transition['prob']\n",
        "                    reward = gw.get_reward(next_state)\n",
        "                    # Calculate the current value\n",
        "                    action_value += prob * (reward + discount * gw.get_value(next_state))\n",
        "                action_values.append(action_value)\n",
        "            # Concider the choice of the best action (max value)\n",
        "            best_value = max(action_values)\n",
        "            delta = max(delta, abs(best_value - gw.get_value(state)))\n",
        "            gw.set_value(state, best_value)\n",
        "        gw.set_next_values()\n",
        "        # Stop when the values difference is less than tolerance\n",
        "        if delta < tolerance:\n",
        "            break\n",
        "    return loops\n",
        "\n",
        "def print_policy(gw):\n",
        "    \"\"\"Use the values found by the algorithm to find the policy and print the grid with actions choosen\"\"\"\n",
        "    policy_grid = [[\"\\t\" for _ in range(gw._width)] for _ in range(gw._height)]\n",
        "    states = [s for s in range(gw._height * gw._width)]\n",
        "    for state in states:\n",
        "        if state in gw._blocked_cells:\n",
        "            policy_grid[gw._state_to_rc(state)[0]][gw._state_to_rc(state)[1]] = \"X\"\n",
        "        elif state == gw._goal_cell:\n",
        "            policy_grid[gw._state_to_rc(state)[0]][gw._state_to_rc(state)[1]] = \"G\"\n",
        "        elif state in gw._danger_cells:\n",
        "            policy_grid[gw._state_to_rc(state)[0]][gw._state_to_rc(state)[1]] = \"D\"\n",
        "        else:\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "            for action in gw.get_actions(state):\n",
        "                action_value = 0\n",
        "                for transition in gw.get_transitions(state, action):\n",
        "                    next_state = transition['state']\n",
        "                    prob = transition['prob']\n",
        "                    reward = gw.get_reward(next_state)\n",
        "                    action_value += prob * (reward + discount * gw.get_value(next_state))\n",
        "                if action_value > best_value:\n",
        "                    best_value = action_value\n",
        "                    best_action = action\n",
        "            policy_grid[gw._state_to_rc(state)[0]][gw._state_to_rc(state)[1]] = action_symbols[best_action]\n",
        "\n",
        "    for row in policy_grid:\n",
        "        print(\" \".join(row))"
      ],
      "metadata": {
        "id": "nxmSqpsRhuHm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "discount = 0.95\n",
        "tolerance = 0.1\n",
        "results = []  # trial, noise, discount, tolerance, iterations"
      ],
      "metadata": {
        "id": "itlc8pqKiacY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset and solve using value iteration\n",
        "simple_gw.reset()\n",
        "print(\"Solving with value iteration (discount_factor=1, noise=0.0):\")\n",
        "iterations = value_iteration(simple_gw, discount=1, tolerance=0.1)\n",
        "print(simple_gw)\n",
        "print(f\"Found in {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(simple_gw)\n",
        "\n",
        "# Save to a results table\n",
        "results.append([1, 0.0, 1, tolerance, iterations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGNTncuSJWe4",
        "outputId": "c994eea7-cb48-4835-cfab-2a2211f8a6fe"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=1, noise=0.0):\n",
            "  5.00   5.00  -5.00   5.00   5.00 \n",
            "  5.00   ----   ----   5.00   5.00 \n",
            "  5.00   ----   ----   5.00   GOAL \n",
            "  5.00   5.00   5.00  -5.00   5.00 \n",
            "  5.00  -5.00   5.00   5.00   5.00 \n",
            "\n",
            "Found in 12 trials.\n",
            "\n",
            "**----------------------**\n",
            "^ ^ D ^ ^\n",
            "^ X X ^ v\n",
            "^ X X > G\n",
            "^ ^ ^ D ^\n",
            "^ D ^ > ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset and solve using value iteration\n",
        "simple_gw.reset()\n",
        "print(\"Solving with value iteration (discount_factor=0.95, noise=0.0):\")\n",
        "iterations = value_iteration(simple_gw, discount, tolerance=0.1)\n",
        "print(simple_gw)\n",
        "print(f\"Found in {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(simple_gw)\n",
        "\n",
        "# Save to a results table\n",
        "results.append([2, 0.0, discount, tolerance, iterations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljcsVQgCYexb",
        "outputId": "755211ec-afef-410d-b13a-d605fc36a50a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.95, noise=0.0):\n",
            "  3.15   2.99  -5.00   4.51   4.75 \n",
            "  3.32   ----   ----   4.75   5.00 \n",
            "  3.49   ----   ----   5.00   GOAL \n",
            "  3.68   3.87   4.07  -5.00   5.00 \n",
            "  3.49  -5.00   4.29   4.51   4.75 \n",
            "\n",
            "Found in 12 trials.\n",
            "\n",
            "**----------------------**\n",
            "v < D > v\n",
            "v X X > v\n",
            "v X X > G\n",
            "> > v D ^\n",
            "^ D > > ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset and solve using value iteration\n",
        "simple_gw.reset()\n",
        "print(\"Solving with value iteration (discount_factor=0.75, noise=0.0):\")\n",
        "iterations = value_iteration(simple_gw, discount=0.75, tolerance=0.1)\n",
        "print(simple_gw)\n",
        "print(f\"Found in {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(simple_gw)\n",
        "\n",
        "results.append([3, 0.0, 0.75, tolerance, iterations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDB6YgKhMrYh",
        "outputId": "5995279b-8b7e-403c-b621-418cd4e1421c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.75, noise=0.0):\n",
            "  0.38   0.28  -5.00   2.81   3.75 \n",
            "  0.50   ----   ----   3.75   5.00 \n",
            "  0.67   ----   ----   5.00   GOAL \n",
            "  0.89   1.19   1.58  -5.00   5.00 \n",
            "  0.67  -5.00   2.11   2.81   3.75 \n",
            "\n",
            "Found in 12 trials.\n",
            "\n",
            "**----------------------**\n",
            "v < D > v\n",
            "v X X > v\n",
            "v X X > G\n",
            "> > v D ^\n",
            "^ D > > ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a noisy grid world"
      ],
      "metadata": {
        "id": "lmv4hKnaMsTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with value iteration (discount_factor=1, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount=1, tolerance=0.1)\n",
        "print(noisy_gw)\n",
        "print(f\"Found in {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(noisy_gw)\n",
        "\n",
        "results.append([4, 0.2, 1, tolerance, iterations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjQ5JmhtM_dz",
        "outputId": "0e976501-3d67-4e80-c2df-0a0f8c453753"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=1, noise=0.2):\n",
            "  1.34   0.78  -5.00   4.21   4.93 \n",
            "  1.43   ----   ----   4.88   4.99 \n",
            "  1.46   ----   ----   4.28   GOAL \n",
            "  1.48   1.52   2.07  -5.00   4.23 \n",
            "  0.97  -5.00   2.71   3.40   4.16 \n",
            "\n",
            "Found in 18 trials.\n",
            "\n",
            "**----------------------**\n",
            "v < D > v\n",
            "v X X > v\n",
            "v X X > G\n",
            "> > v D ^\n",
            "^ D > > ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with value iteration (discount_factor=0.95, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount, tolerance=0.1)\n",
        "print(noisy_gw)\n",
        "print(f\"Found in {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(noisy_gw)\n",
        "# optimum policy found\n",
        "\n",
        "results.append([5, 0.2, 0.95, tolerance, iterations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4sPRq8xG6fg",
        "outputId": "8dcd8c48-3fce-469e-fc2a-155594a90bdd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.95, noise=0.2):\n",
            "  0.42  -0.10  -5.00   3.60   4.51 \n",
            "  0.56   ----   ----   4.49   4.88 \n",
            "  0.65   ----   ----   4.22   GOAL \n",
            "  0.72   0.83   1.40  -5.00   4.17 \n",
            "  0.23  -5.00   2.09   2.90   3.84 \n",
            "\n",
            "Found in 15 trials.\n",
            "\n",
            "**----------------------**\n",
            "v < D > v\n",
            "v X X > v\n",
            "v X X > G\n",
            "> > v D ^\n",
            "^ D > > ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For a noisy grid world  --trial 2\n",
        "print(\"Solving with value iteration (discount_factor=0.75, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount=0.75, tolerance=0.1)\n",
        "print(noisy_gw)\n",
        "print(f\"Found in {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(noisy_gw)\n",
        "# Bad Policy\n",
        "\n",
        "results.append([6, 0.2, 0.75, tolerance, iterations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6tsRb6Ph1Xo",
        "outputId": "7f680921-8c86-4e76-a59a-7a280e974a60"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.75, noise=0.2):\n",
            " -0.02  -0.39  -5.00   1.81   3.12 \n",
            " -0.00   ----   ----   3.17   4.54 \n",
            " -0.00   ----   ----   4.03   GOAL \n",
            " -0.04  -0.39  -0.08  -5.00   4.00 \n",
            " -0.40  -5.00   0.51   1.40   2.74 \n",
            "\n",
            "Found in 7 trials.\n",
            "\n",
            "**----------------------**\n",
            "v < D v v\n",
            "> X X > v\n",
            "^ X X > G\n",
            "^ < v D ^\n",
            "^ D > > ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For a noisy grid world  --trial 3 smaller tolerance\n",
        "print(\"Solving with value iteration (discount_factor=0.75, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount=0.75, tolerance=0.001)\n",
        "print(noisy_gw)\n",
        "print(f\"Found in {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(noisy_gw)\n",
        "\n",
        "results.append([7, 0.2, 0.75, 0.001, iterations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6x5LbZ_nVdz",
        "outputId": "8a7bacdf-cbfe-430d-f1ae-292018365413"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.75, noise=0.2):\n",
            " -0.02  -0.39  -5.00   1.82   3.13 \n",
            " -0.00   ----   ----   3.18   4.54 \n",
            " -0.01   ----   ----   4.03   GOAL \n",
            " -0.04  -0.37  -0.03  -5.00   4.00 \n",
            " -0.40  -5.00   0.54   1.41   2.75 \n",
            "\n",
            "Found in 13 trials.\n",
            "\n",
            "**----------------------**\n",
            "v < D v v\n",
            "> X X > v\n",
            "^ X X > G\n",
            "^ > v D ^\n",
            "^ D > > ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"#Trial\", \"Noise\", \"Discount\", \"Tolerance\", \"Iterations\"]\n",
        "\n",
        "# Generate and print the table\n",
        "print(tabulate(results, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTJ0mDcFQCvh",
        "outputId": "d1c52185-1389-4380-8bb2-87ea054716c4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+-------------+--------------+\n",
            "|   #Trial |   Noise |   Discount |   Tolerance |   Iterations |\n",
            "+==========+=========+============+=============+==============+\n",
            "|        1 |     0   |       1    |       0.1   |           12 |\n",
            "+----------+---------+------------+-------------+--------------+\n",
            "|        2 |     0   |       0.95 |       0.1   |           12 |\n",
            "+----------+---------+------------+-------------+--------------+\n",
            "|        3 |     0   |       0.75 |       0.1   |           12 |\n",
            "+----------+---------+------------+-------------+--------------+\n",
            "|        4 |     0.2 |       1    |       0.1   |           18 |\n",
            "+----------+---------+------------+-------------+--------------+\n",
            "|        5 |     0.2 |       0.95 |       0.1   |           15 |\n",
            "+----------+---------+------------+-------------+--------------+\n",
            "|        6 |     0.2 |       0.75 |       0.1   |            7 |\n",
            "+----------+---------+------------+-------------+--------------+\n",
            "|        7 |     0.2 |       0.75 |       0.001 |           13 |\n",
            "+----------+---------+------------+-------------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare using: different discount factors, noise values, number of iterations, tolerance values"
      ],
      "metadata": {
        "id": "JElaoCdtJVYm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRSUzBc_JOvv"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}