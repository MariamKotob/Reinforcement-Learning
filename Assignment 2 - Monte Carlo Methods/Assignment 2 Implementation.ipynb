{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmbSb4ZFgQrs"
      },
      "source": [
        "### Initial Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-SOCfgJtHJw",
        "outputId": "379a16c9-ff2e-4b4d-de3b-91c7d6fe8802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b59040de82a7>:10: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-notebook')\n",
            "<ipython-input-6-b59040de82a7>:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-whitegrid')\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import timeit\n",
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s0cEpP6BtPSO"
      },
      "outputs": [],
      "source": [
        "class Action(IntEnum):\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "action_to_str = {\n",
        "    Action.up : \"up\",\n",
        "    Action.right : \"right\",\n",
        "    Action.down : \"down\",\n",
        "    Action.left : \"left\",\n",
        "}\n",
        "\n",
        "action_to_offset = {\n",
        "    Action.up : (-1, 0),\n",
        "    Action.right : (0, 1),\n",
        "    Action.down : (1, 0),\n",
        "    Action.left : (0, -1),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN2_qvsIgWss"
      },
      "source": [
        "### Grid World Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wL9WYBulgbZB"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, height, width, goal, start, danger, blocked, goal_value=1.0,  danger_value=-1.0, noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "\n",
        "        Parameters:\n",
        "        - height (int): Number of rows.\n",
        "        - width (int): Number of columns.\n",
        "        - goal (int): Index number of the goal cell.\n",
        "        - goal_value (float): Reward given for the goal cell.\n",
        "        - danger (list of int): Indices of cells marked as danger.\n",
        "        - danger_value (float): Reward given for danger cell.\n",
        "        - blocked (list of int): Indices of cells marked as blocked (cannot enter).\n",
        "        - noise (float): Probability of resulting state not being what was expected.\n",
        "        \"\"\"\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = np.zeros(height * width)\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._start_cell = start\n",
        "        self._noise = noise\n",
        "        assert 0 <= noise < 1  # Ensure valid noise value\n",
        "        self.create_next_values()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = np.zeros(self._height * self._width)\n",
        "        self.create_next_values()\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return 0 <= state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return 0 <= state_r < self._height and 0 <= state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        dr, dc = action_to_offset[action]\n",
        "        new_r, new_c = self._state_to_rc(state)\n",
        "        new_r += dr\n",
        "        new_c += dc\n",
        "        new_state = new_r * self._width + new_c\n",
        "\n",
        "        if self._inbounds(new_state) and new_state != self._blocked_cells:\n",
        "            return new_state\n",
        "        return state\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal or danger).\n",
        "        \"\"\"\n",
        "        return state == self._goal_cell or state == self._danger_cells\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment.\n",
        "        \"\"\"\n",
        "        return [s for s in range(self._width * self._height) if not self.is_terminal(s) and s != self._blocked_cells]\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state.\n",
        "        \"\"\"\n",
        "        actions = []\n",
        "        for action in Action:\n",
        "            next_state = self._state_from_action(state, action)\n",
        "            if next_state != state:\n",
        "                actions.append(action)\n",
        "        return actions\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state.\n",
        "        \"\"\"\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        elif state == self._danger_cells:\n",
        "            return self._danger_value\n",
        "        else:\n",
        "            return -0.1\n",
        "\n",
        "    def get_transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Get a list of transitions as a result of attempting the action in the current state.\n",
        "        Each item in the list is a tuple containing the probability of reaching that state and the next state itself.\n",
        "        \"\"\"\n",
        "        next_state = self._state_from_action(state, action)\n",
        "        if next_state == state:\n",
        "            return [(1.0, state)]\n",
        "        return [(1.0 - self._noise, next_state), (self._noise, state)]\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Get the current value of the state.\n",
        "        \"\"\"\n",
        "        return self._grid_values[state]\n",
        "\n",
        "    def create_next_values(self):\n",
        "        \"\"\"\n",
        "        Creates a temporary storage for state value updating.\n",
        "        \"\"\"\n",
        "        self._next_values = np.zeros_like(self._grid_values)\n",
        "\n",
        "    def set_next_values(self):\n",
        "        \"\"\"\n",
        "        Set the state values from the temporary copied values.\n",
        "        \"\"\"\n",
        "        self._grid_values = np.copy(self._next_values)\n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        \"\"\"\n",
        "        Set the value of the state into the temporary copy.\n",
        "        This value will not update into the main storage until self.set_next_values() is called.\n",
        "        \"\"\"\n",
        "        self._next_values[state] = value\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values.\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell == self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell == self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str\n",
        "\n",
        "    def print_policy(self, Q):\n",
        "        \"\"\"\n",
        "        Pretty print the policy values.\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell == self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell == self._danger_cells:\n",
        "                    out_str += \"{:>6}\".format(\"DANGR\")\n",
        "                else:\n",
        "                    if cell in Q:\n",
        "                        best_action = np.argmax(Q[cell])\n",
        "                        out_str += \"{:>6}\".format(action_to_str[best_action])\n",
        "                    else:\n",
        "                        out_str += \"{:>6}\".format(\"????\")\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        print(out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0iGbTt7C0lz",
        "outputId": "bedf8363-46af-4333-f6c5-685ab0fe1709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0.00   0.00   0.00   GOAL \n",
            "  0.00   ----   0.00  -1.00 \n",
            "  0.00   0.00   0.00   0.00 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=3, width=4, goal=3, start=8, danger=7, blocked=5, noise=0.0)\n",
        "print(simple_gw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90BYjJk6Uek"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ghGtyjcgoux"
      },
      "source": [
        "### Algorithms Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### On-policy MC with exploring starts (10 points)"
      ],
      "metadata": {
        "id": "IIlOoFDmxcs0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dIJQTfK46YH5"
      },
      "outputs": [],
      "source": [
        "def on_policy_mc_with_exploring_starts(env, num_episodes, gamma=1.0):\n",
        "    \"\"\"Implement the on policy MC with exploring starts Algorithm\"\"\"\n",
        "    # intialize variables to count the average\n",
        "    Q = defaultdict(lambda: np.zeros(len(Action)))\n",
        "    returns_sum = defaultdict(lambda: np.zeros(len(Action)))\n",
        "    returns_count = defaultdict(lambda: np.zeros(len(Action)))\n",
        "\n",
        "    def policy(state):\n",
        "        # epsilon greedy to find the next action\n",
        "        if np.random.rand() < 0.1:\n",
        "            return np.random.choice(list(Action))\n",
        "        else:\n",
        "            return np.argmax(Q[state])\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # Generating episodes with exploring starts\n",
        "        state = np.random.choice(env.get_states())\n",
        "        episode = []\n",
        "        while not env.is_terminal(state):\n",
        "            # select the state & action chosen and add the pair\n",
        "            action = policy(state)\n",
        "            next_state = env._state_from_action(state, action)\n",
        "            reward = env.get_reward(next_state)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0  # G - discounted return\n",
        "        # Iterate over trajectory in reverse\n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            # calculate the average returns and update the Q values and policy\n",
        "            state, action, reward = episode[t]\n",
        "            G = reward + gamma * G\n",
        "            if (state, action) not in [(ep_state, ep_action) for ep_state, ep_action, _ in episode[:t]]:\n",
        "                # condition to use first visit evaluation\n",
        "                returns_sum[state][action] += G\n",
        "                returns_count[state][action] += 1.0\n",
        "                Q[state][action] = returns_sum[state][action] / returns_count[state][action]\n",
        "\n",
        "    return Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B3QlAfgEMUU",
        "outputId": "2e7db826-8b6f-4b48-b72c-c74b5cda9db7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy MC with Exploring Starts:\n",
            "State 0: right\n",
            "State 1: right\n",
            "State 2: right\n",
            "State 4: left\n",
            "State 6: up\n",
            "State 8: right\n",
            "State 9: right\n",
            "State 10: up\n",
            "State 11: left\n"
          ]
        }
      ],
      "source": [
        "print(\"On-policy MC with Exploring Starts:\")\n",
        "Q_on_policy_exploring_starts = on_policy_mc_with_exploring_starts(simple_gw, num_episodes=10000)\n",
        "\n",
        "for state in range(simple_gw._height * simple_gw._width):\n",
        "    if state in simple_gw.get_states():\n",
        "        print(f\"State {state}: {action_to_str[np.argmax(Q_on_policy_exploring_starts[state])]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLAmU6Dag0-3",
        "outputId": "0d4c0449-d161-4ce8-c346-0448b303a1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " right  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            " right  right     up   left \n",
            "\n"
          ]
        }
      ],
      "source": [
        "simple_gw.print_policy(Q_on_policy_exploring_starts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiments"
      ],
      "metadata": {
        "id": "QONMcOVqWnfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsides = [10000, 50000, 100000]\n",
        "gamma = 1.0\n",
        "es_results = []\n",
        "\n",
        "for i, n_episodes in enumerate(epsides):\n",
        "    print(f\"On-policy MC with Exploring Starts for {n_episodes} episodes:\")\n",
        "    Q_on_policy_exploring_starts = on_policy_mc_with_exploring_starts(simple_gw, num_episodes=n_episodes, gamma=gamma)\n",
        "    simple_gw.print_policy(Q_on_policy_exploring_starts)\n",
        "    es_results.append((i, n_episodes, gamma))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHX7-Z99JUQ7",
        "outputId": "65c42ef9-f849-43c8-bf34-3e9f7ba20244"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy MC with Exploring Starts for 10000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            " right  right     up   left \n",
            "\n",
            "\n",
            "On-policy MC with Exploring Starts for 50000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "On-policy MC with Exploring Starts for 100000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up  right     up   left \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Gamma= [0.9, 0.95, 1.1]\n",
        "n_episodes = 10000\n",
        "\n",
        "for i, gamma in enumerate(Gamma):\n",
        "    print(f\"On-policy MC with Exploring Starts for {gamma} episodes:\")\n",
        "    Q_on_policy_exploring_starts = on_policy_mc_with_exploring_starts(simple_gw, num_episodes=n_episodes, gamma=gamma)\n",
        "    simple_gw.print_policy(Q_on_policy_exploring_starts)\n",
        "    es_results.append((i, n_episodes, gamma))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n_2Ys7UXb8o",
        "outputId": "72550989-1619-425f-fec7-22e4d7e32a56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy MC with Exploring Starts for 0.9 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "On-policy MC with Exploring Starts for 0.95 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "On-policy MC with Exploring Starts for 1.1 episodes:\n",
            " right  right  right   GOAL \n",
            "  left   ----  right  DANGR \n",
            "  left   left     up     up \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"#Trial\", \"#Episodes\", \"Gamma\"]\n",
        "print(tabulate(es_results, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUe1xhYYYb_v",
        "outputId": "b69b06f3-36a9-4fd0-f377-53ab989aed2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------+\n",
            "|   #Trial |   #Episodes |   Gamma |\n",
            "+==========+=============+=========+\n",
            "|        0 |       10000 |    1    |\n",
            "+----------+-------------+---------+\n",
            "|        1 |       50000 |    1    |\n",
            "+----------+-------------+---------+\n",
            "|        2 |      100000 |    1    |\n",
            "+----------+-------------+---------+\n",
            "|        0 |       10000 |    0.9  |\n",
            "+----------+-------------+---------+\n",
            "|        1 |       10000 |    0.95 |\n",
            "+----------+-------------+---------+\n",
            "|        2 |       10000 |    1.1  |\n",
            "+----------+-------------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhEEW168hWux"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### On-policy MC control without exploring starts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vMgsX8Ql_sGp"
      },
      "outputs": [],
      "source": [
        "def on_policy_mc_control(env, num_episodes, gamma=1.0, epsilon=0.1):\n",
        "    \"\"\" returns the Q function, which approximates the expected return for each state-action pair\"\"\"\n",
        "    # Initialize Q, C\n",
        "    env.reset()\n",
        "    Q = defaultdict(lambda: np.zeros(len(Action)))\n",
        "    returns_sum = defaultdict(lambda: np.zeros(len(Action)))\n",
        "    returns_count = defaultdict(lambda: np.zeros(len(Action)))\n",
        "\n",
        "    def policy(state):\n",
        "        # Deinfe the epsioln greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(list(Action))\n",
        "        else:\n",
        "            return np.argmax(Q[state])\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # always start from the same state (8)\n",
        "        state = 8\n",
        "        episode = []\n",
        "        while not env.is_terminal(state):\n",
        "            action = policy(state)\n",
        "            next_state = env._state_from_action(state, action)\n",
        "            reward = env.get_reward(next_state)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0  # G - discounted return\n",
        "        # Iterate over trajectory in reverse\n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            G = reward + gamma * G\n",
        "            # condition to use first visit evaluation\n",
        "            if (state, action) not in [(ep_state, ep_action) for ep_state, ep_action, _ in episode[:t]]:\n",
        "                returns_sum[state][action] += G\n",
        "                returns_count[state][action] += 1.0\n",
        "                # Update Q values\n",
        "                Q[state][action] = returns_sum[state][action] / returns_count[state][action]\n",
        "\n",
        "    return Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI73wrCNENeT",
        "outputId": "a50a186c-aeb2-43eb-a997-941e5a8ee183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy MC Control without Exploring Starts:\n",
            "State 0: down\n",
            "State 1: right\n",
            "State 2: right\n",
            "State 4: left\n",
            "State 6: up\n",
            "State 8: up\n",
            "State 9: right\n",
            "State 10: up\n",
            "State 11: left\n"
          ]
        }
      ],
      "source": [
        "print(\"On-policy MC Control without Exploring Starts:\")\n",
        "Q_on_policy_control = on_policy_mc_control(simple_gw, num_episodes=10000)\n",
        "for state in range(simple_gw._height * simple_gw._width):\n",
        "    if state in simple_gw.get_states():\n",
        "        print(f\"State {state}: {action_to_str[np.argmax(Q_on_policy_control[state])]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwGW6V3f_PV1",
        "outputId": "fdf5ca0f-814a-4a2d-876d-a04859eb3fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up  right     up   left \n",
            "\n"
          ]
        }
      ],
      "source": [
        "simple_gw.print_policy(Q_on_policy_control)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiments"
      ],
      "metadata": {
        "id": "IFXWuFxtZpIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsides = [10000, 50000, 100000]\n",
        "Gammas= [0.9, 0.95, 1.1]\n",
        "Epsilons= [0.01, 0.2, 0.3]\n",
        "control_results = []\n",
        "\n",
        "gamma = 1.0\n",
        "ep = 0.1\n",
        "for i, n_episodes in enumerate(epsides):\n",
        "    print(f\"On-policy MC Control without Exploring Starts for {n_episodes} episodes:\")\n",
        "    Q_on_policy_control = on_policy_mc_control(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_on_policy_control)\n",
        "    control_results.append((i, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkBN6hdxZofo",
        "outputId": "90d62af9-d394-4bf6-c5cd-4773e1f6bcff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy MC Control without Exploring Starts for 10000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up  right     up   left \n",
            "\n",
            "\n",
            "On-policy MC Control without Exploring Starts for 50000 episodes:\n",
            " right  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up     up \n",
            "\n",
            "\n",
            "On-policy MC Control without Exploring Starts for 100000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up  right     up   left \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 10000\n",
        "\n",
        "for i, gamma in enumerate(Gammas):\n",
        "    print(f\"On-policy MC Control without Exploring Starts for gamma = {gamma}:\")\n",
        "    Q_on_policy_control = on_policy_mc_control(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_on_policy_control)\n",
        "    control_results.append((i+3, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZc3X7P-bC7q",
        "outputId": "816efb78-7ed6-40d0-f932-7f5fc67a07e3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy MC Control without Exploring Starts for gamma = 0.9:\n",
            " right  right  right   GOAL \n",
            "  left   ----   down  DANGR \n",
            "    up   left   left  right \n",
            "\n",
            "\n",
            "On-policy MC Control without Exploring Starts for gamma = 0.95:\n",
            "  down   left  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left   left  right \n",
            "\n",
            "\n",
            "On-policy MC Control without Exploring Starts for gamma = 1.1:\n",
            " right   down  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "  left  right     up   down \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1.0\n",
        "\n",
        "for i, ep in enumerate(Epsilons):\n",
        "    print(f\"On-policy MC Control without Exploring Starts for epsilon = {ep}:\")\n",
        "    Q_on_policy_control = on_policy_mc_control(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_on_policy_control)\n",
        "    control_results.append((i+6, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3_7-jKSbQiB",
        "outputId": "ae4a7c57-5ea4-4636-db55-957a2624a52a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy MC Control without Exploring Starts for epsilon = 0.01:\n",
            "  left  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "  left  right     up  right \n",
            "\n",
            "\n",
            "On-policy MC Control without Exploring Starts for epsilon = 0.2:\n",
            "  down   left  right   GOAL \n",
            "  left   ----  right  DANGR \n",
            "    up   left   left   down \n",
            "\n",
            "\n",
            "On-policy MC Control without Exploring Starts for epsilon = 0.3:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"#Trial\", \"#Episodes\", \"Gammas\", \"Epsilons\"]\n",
        "print(tabulate(control_results, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MczL2HLaUeN",
        "outputId": "90825a24-813b-432c-b1c8-d93020bea0ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------+------------+\n",
            "|   #Trial |   #Episodes |   Gammas |   Epsilons |\n",
            "+==========+=============+==========+============+\n",
            "|        0 |       10000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        1 |       50000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        2 |      100000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        3 |       10000 |     0.9  |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        4 |       10000 |     0.95 |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        5 |       10000 |     1.1  |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        6 |       10000 |     1    |       0.01 |\n",
            "+----------+-------------+----------+------------+\n",
            "|        7 |       10000 |     1    |       0.2  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        8 |       10000 |     1    |       0.3  |\n",
            "+----------+-------------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMaEniPY_TBX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### Off-policy MC prediction (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the epsilon-greedy policy\n",
        "def epsilon_greedy_policy(Q, state, epsilon=0.1, n_actions=4):\n",
        "    A = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
        "    best_action = np.argmax(Q[state])\n",
        "    A[best_action] += (1.0 - epsilon)\n",
        "    return A\n",
        "\n",
        "# Off-policy MC prediction\n",
        "def off_policy_mc_prediction(env, num_episodes, gamma=1.0, epsilon=0.1):\n",
        "    env.reset()\n",
        "    # Initialize Q and C arbitrarily\n",
        "    Q = defaultdict(lambda: np.zeros(4))\n",
        "    C = defaultdict(lambda: np.zeros(4))\n",
        "\n",
        "    # Generate episodes\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Generate an episode using behavior policy (uniform random policy)\n",
        "        episode = []\n",
        "        state = env._start_cell\n",
        "        while True:\n",
        "            probs = np.ones(4) / 4\n",
        "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "            _, next_state = env.get_transitions(state, Action(action))[0]\n",
        "            reward = env.get_reward(next_state)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            if env.is_terminal(state):\n",
        "                break\n",
        "\n",
        "        # Initialize variables\n",
        "        G = 0\n",
        "        W = 1\n",
        "\n",
        "        # Iterate over trajectory in reverse\n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            C[state][action] += W\n",
        "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
        "            if action != np.argmax(epsilon_greedy_policy(Q, state, epsilon)):\n",
        "                break\n",
        "            W *= 1.0 / (0.25)\n",
        "\n",
        "    return Q\n"
      ],
      "metadata": {
        "id": "8lhMfg0N6K4X"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_off_policy = off_policy_mc_prediction(simple_gw, num_episodes=10000)\n",
        "\n",
        "print(\"Q-values:\")\n",
        "for state in range(simple_gw._width * simple_gw._height):\n",
        "    if state in Q_off_policy:\n",
        "        print(f\"State {state}: {Q_off_policy[state]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hoj_guf7PCAo",
        "outputId": "df0dfbf0-956c-4b41-89b3-25afd73fc39e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values:\n",
            "State 0: [0.8 0.8 0.9 0.8]\n",
            "State 1: [0.8 0.9 0.8 0.8]\n",
            "State 2: [0.9 1.  0.8 0.8]\n",
            "State 4: [0.7974359 0.9       0.8       1.       ]\n",
            "State 6: [ 0.9 -1.   0.7  0.8]\n",
            "State 8: [ 0.9  0.7  0.8 -1. ]\n",
            "State 9: [0.7 0.7 0.7 0.8]\n",
            "State 10: [0.8 0.6 0.7 0.7]\n",
            "State 11: [-1.   0.6  0.6  0.7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_gw.print_policy(Q_off_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VeunaMaO73-",
        "outputId": "44d1de78-57c2-4be8-a1ae-a6911c9f7664"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiments"
      ],
      "metadata": {
        "id": "DhVYpAAjc4cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsides = [10000, 50000, 100000]\n",
        "Gammas= [0.9, 0.95, 1.1]\n",
        "Epsilons= [0.01, 0.2, 0.3]\n",
        "control_results = []\n",
        "\n",
        "gamma = 1.0\n",
        "ep = 0.1\n",
        "for i, n_episodes in enumerate(epsides):\n",
        "    print(f\"Off-policy MC Prediction for {n_episodes} episodes:\")\n",
        "    Q_off_policy = off_policy_mc_prediction(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_off_policy)\n",
        "    control_results.append((i, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0xjHocUc7lk",
        "outputId": "d898dd75-68ec-4a88-8e38-c92f9b996ffe"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Off-policy MC Prediction for 10000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Prediction for 50000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Prediction for 100000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 10000\n",
        "\n",
        "for i, gamma in enumerate(Gammas):\n",
        "    print(f\"Off-policy MC Prediction for gamma = {gamma}: \")\n",
        "    Q_off_policy = off_policy_mc_prediction(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_off_policy)\n",
        "    control_results.append((i+3, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAqgxakydbk9",
        "outputId": "e4c04c15-f5ee-485c-ba5e-3a8e8d6f9b86"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Off-policy MC Prediction for gamma = 0.9: \n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Prediction for gamma = 0.95: \n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Prediction for gamma = 1.1: \n",
            "  down   ????     up   GOAL \n",
            "    up   ----     up  DANGR \n",
            "    up   ????     up  right \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 100000\n",
        "gamma = 1\n",
        "\n",
        "for i, ep in enumerate(Epsilons):\n",
        "    print(f\"Off-policy MC Prediction for epsilon = {ep}: \")\n",
        "    Q_off_policy = off_policy_mc_prediction(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_off_policy)\n",
        "    control_results.append((i+6, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGwWh-z-eU6X",
        "outputId": "94c9e80e-5a48-4798-df83-d20a488d20f6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Off-policy MC Prediction for epsilon = 0.01: \n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Prediction for epsilon = 0.2: \n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Prediction for epsilon = 0.3: \n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"#Trial\", \"#Episodes\", \"Gammas\", \"Epsilons\"]\n",
        "print(tabulate(control_results, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7BIPbfvdD-C",
        "outputId": "eaacf66e-3379-4e30-b638-5c451acf79ce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------+------------+\n",
            "|   #Trial |   #Episodes |   Gammas |   Epsilons |\n",
            "+==========+=============+==========+============+\n",
            "|        0 |       10000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        1 |       50000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        2 |      100000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        3 |      100000 |     0.9  |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        4 |      100000 |     0.95 |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        5 |      100000 |     1.1  |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        6 |      100000 |     1    |       0.01 |\n",
            "+----------+-------------+----------+------------+\n",
            "|        7 |      100000 |     1    |       0.2  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        8 |      100000 |     1    |       0.3  |\n",
            "+----------+-------------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPLe238CDids"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### Off-policy MC control (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an episode using the behavior policy\n",
        "def generate_episode(env):\n",
        "    episode = []\n",
        "    state = env._start_cell\n",
        "    while not env.is_terminal(state):\n",
        "        probs = np.ones(4) / 4  # Uniform random policy\n",
        "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "        transitions = env.get_transitions(state, action)\n",
        "        next_state = transitions[0][1]\n",
        "        reward = env.get_reward(next_state)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode"
      ],
      "metadata": {
        "id": "wetcxo-fZcYC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def off_policy_mc_control(env, num_episodes, gamma=1.0, epsilon=0.1):\n",
        "    env.reset()\n",
        "    Q = defaultdict(lambda: np.zeros(4))\n",
        "    C = defaultdict(lambda: np.zeros(4))\n",
        "    policy = {}\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        episode = generate_episode(env)\n",
        "\n",
        "        G = 0\n",
        "        W = 1\n",
        "\n",
        "        # Iterate over trajectory in reverse\n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            C[state][action] += W\n",
        "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
        "            policy[state] = np.argmax(Q[state])\n",
        "            if action != policy[state]:\n",
        "                break\n",
        "            W *= 1.0 / (1.0 / len(env.get_actions(state)))\n",
        "\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "p8F7t_-sZfvJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_off_policy_control, policy = off_policy_mc_control(simple_gw, num_episodes=10000)\n",
        "\n",
        "print(\"Q-values:\")\n",
        "for state in range(simple_gw._width * simple_gw._height):\n",
        "    if state in Q_off_policy_control:\n",
        "        print(f\"State {state}: {Q_off_policy_control[state]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzErS4sCZkZG",
        "outputId": "283ebbf6-293d-45b2-9f4e-2ac3a9939043"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values:\n",
            "State 0: [0.8 0.8 0.9 0.8]\n",
            "State 1: [0.8 0.9 0.8 0.8]\n",
            "State 2: [0.9 1.  0.8 0.8]\n",
            "State 4: [0.8 0.9 0.8 1. ]\n",
            "State 6: [ 0.9 -1.   0.7  0.8]\n",
            "State 8: [ 0.9  0.7  0.8 -1. ]\n",
            "State 9: [0.7 0.7 0.7 0.8]\n",
            "State 10: [0.8 0.6 0.7 0.7]\n",
            "State 11: [-1.   0.6  0.6  0.7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op9YTz1jNWAi",
        "outputId": "5b45d7c7-d98f-4db7-ea3e-545f1d5cb719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n"
          ]
        }
      ],
      "source": [
        "simple_gw.print_policy(Q_off_policy_control)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiments"
      ],
      "metadata": {
        "id": "B69JWXZIfSaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsides = [10000, 50000, 100000]\n",
        "Gammas= [0.9, 0.95, 1.1]\n",
        "Epsilons= [0.01, 0.2, 0.3]\n",
        "control_results = []\n",
        "\n",
        "gamma = 1.0\n",
        "ep = 0.1\n",
        "for i, n_episodes in enumerate(epsides):\n",
        "    print(f\"Off-policy MC Control for {n_episodes} episodes:\")\n",
        "    Q_off_policy_control, policy = off_policy_mc_control(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_off_policy_control)\n",
        "    control_results.append((i, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YOCEoJZfUVI",
        "outputId": "8709b0b0-b807-46ae-ea4a-602a717058bf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Off-policy MC Control for 10000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Control for 50000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Control for 100000 episodes:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 1000\n",
        "\n",
        "for i, gamma in enumerate(Gammas):\n",
        "    print(f\"Off-policy MC Control for gamma = {gamma}:\")\n",
        "    Q_off_policy_control, policy = off_policy_mc_control(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_off_policy_control)\n",
        "    control_results.append((i+3, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRzONW35f4h7",
        "outputId": "35d63b6c-967a-4d67-ea0a-2363d8371bff"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Off-policy MC Control for gamma = 0.9:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up  right \n",
            "\n",
            "\n",
            "Off-policy MC Control for gamma = 0.95:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Control for gamma = 1.1:\n",
            "  ????   ????     up   GOAL \n",
            " right   ----     up  DANGR \n",
            "    up     up   ????  right \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1\n",
        "n_episodes = 50000\n",
        "\n",
        "for i, ep in enumerate(Epsilons):\n",
        "    print(f\"Off-policy MC Control for Epsilon = {ep}:\")\n",
        "    Q_off_policy_control, policy = off_policy_mc_control(simple_gw, num_episodes=n_episodes, gamma=gamma, epsilon=ep)\n",
        "    simple_gw.print_policy(Q_off_policy_control)\n",
        "    control_results.append((i+6, n_episodes, gamma, ep))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LReqd9fMgC-r",
        "outputId": "44aa7e18-1f05-4896-dc4e-a9f5e6c54f2c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Off-policy MC Control for Epsilon = 0.01:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Control for Epsilon = 0.2:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n",
            "Off-policy MC Control for Epsilon = 0.3:\n",
            "  down  right  right   GOAL \n",
            "  left   ----     up  DANGR \n",
            "    up   left     up   left \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"#Trial\", \"#Episodes\", \"Gammas\", \"Epsilons\"]\n",
        "print(tabulate(control_results, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzQh6WRCgKAc",
        "outputId": "bd0314fe-779e-4d95-f932-fb6a741ef45f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------+------------+\n",
            "|   #Trial |   #Episodes |   Gammas |   Epsilons |\n",
            "+==========+=============+==========+============+\n",
            "|        0 |       10000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        1 |       50000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        2 |      100000 |     1    |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        3 |        1000 |     0.9  |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        4 |        1000 |     0.95 |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        5 |        1000 |     1.1  |       0.1  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        6 |       50000 |     1    |       0.01 |\n",
            "+----------+-------------+----------+------------+\n",
            "|        7 |       50000 |     1    |       0.2  |\n",
            "+----------+-------------+----------+------------+\n",
            "|        8 |       50000 |     1    |       0.3  |\n",
            "+----------+-------------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlPzEqXNCzHr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resources & Refrences\n",
        "* https://github.com/nums11/rl/tree/main/chapter_5_monte_carlo"
      ],
      "metadata": {
        "id": "yOw0I3tKb57D"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}